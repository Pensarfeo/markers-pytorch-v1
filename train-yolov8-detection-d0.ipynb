{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f15ecf6d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8bc3ad73",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.optim import lr_scheduler, RMSprop\n",
    "import torch.nn.functional as F\n",
    "    \n",
    "# import utils\n",
    "from importlib import reload  # Python 3.4+\n",
    "import addSrcToPath\n",
    "\n",
    "from dataSaver import DataSaver\n",
    "\n",
    "\n",
    "import dataloader as __dataloader\n",
    "\n",
    "reload(__dataloader)\n",
    "\n",
    "from dataloader import DataProvider\n",
    "\n",
    "from utils.dice_score import dice_loss\n",
    "\n",
    "from utils.math import MovingAverage\n",
    "from utils.tensor import makeDownscaler\n",
    "\n",
    "# import the time module\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498ab01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTES\n",
    "# Notes This model's optimal learning rate was between 0.0004 and 0.006.\n",
    "# lower learning rate was better with 0.8 being much better than all the others\n",
    "# I was training only on the dot's points l2 for xywh and huber loss for the class\n",
    "# I was summing the results ber image and averaging the batch.\n",
    "# The class detection was further normalized\n",
    "# Manager to cross the treshold of 0.1 treshold\n",
    "# The problem was that the results on real images are supre crappy.\n",
    "\n",
    "# I need to try to bring back the training over all points for xywh\n",
    "# use mean results for everything.\n",
    "# I need to make a real dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53091ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ba5adc0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# device = (\n",
    "#     \"cuda\"\n",
    "#     if torch.cuda.is_available()\n",
    "#     else \"mps\"\n",
    "#     if torch.backends.mps.is_available()\n",
    "#     else \"cpu\"\n",
    "# )\n",
    "\n",
    "# def getCheckpointName(modelName, epoch):\n",
    "#   return os.path.join(modelName, f'epochs-{epoch}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67e86249",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def getCheckpointName(modelName, epoch):\n",
    "  return os.path.join(modelName, f'epochs-{epoch}.pt')\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "annotationsDir = os.path.join('.', 'data', 'coco', 'train2017')\n",
    "modelName = os.path.join('outputs', \"Yolov8-cocoTest-noise-brighness-colortwicking-counterExamples-d0-test-lr\")\n",
    "\n",
    "n = 1\n",
    "# batchSize = 92\n",
    "batchSize = 64//n\n",
    "\n",
    "learningRate = 0.01\n",
    "weightDecay = 1e-4\n",
    "momentum = 0.935\n",
    "amp = True\n",
    "\n",
    "accumulate = 64//batchSize\n",
    "totIters = 1276\n",
    "\n",
    "model = None\n",
    "checkpoint = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1bb3543b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<module 'model' from 'D:\\\\Projects\\\\kineticLab\\\\models\\\\demo\\\\src\\\\model\\\\__init__.py'>\n",
      "@@@@@ No checkpoint found for epoch 3\n",
      "@@@@@ Initializing model from scratch\n",
      "=> model size: 16.137MB\n",
      "currentEpoch 0\n",
      "training for: 2 epochs\n"
     ]
    }
   ],
   "source": [
    "from model.yolov8.semantic_detection_d0 import Model\n",
    "import model as _model\n",
    "reload(_model)\n",
    "print(_model)\n",
    "\n",
    "model = Model(n)\n",
    "epoch, checkpoint = _model.initialize(model, modelName, n=3)\n",
    "model.to(device);\n",
    "\n",
    "# Initialize Epoch\n",
    "currentEpoch = epoch + 1\n",
    "epochs = 2\n",
    "print('currentEpoch', currentEpoch)\n",
    "print('training for:', epochs, 'epochs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad612d87",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Init dataloader\n",
    "dataProvider = DataProvider(annotationsDir)\n",
    "totIters = dataProvider.dataset.__len__()//batchSize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f945d862",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# norm255 = 1/255\n",
    "# downScaler8c1 = makeDownscaler(d = 8).to(device)\n",
    "# downScaler2c1 = makeDownscaler(d = 2).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "09986ede",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import optimizer as _optimizer\n",
    "reload(_optimizer)\n",
    "\n",
    "optimizer = _optimizer.build(\n",
    "    model,\n",
    "    lr=learningRate,\n",
    "    weightDecay=weightDecay,\n",
    "    momentum=momentum,\n",
    ")\n",
    "\n",
    "if (checkpoint and checkpoint['optimizer_state_dict']):\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    " \n",
    "before_lr = optimizer.param_groups[0][\"lr\"]\n",
    "grad_scaler = torch.cuda.amp.GradScaler(enabled=amp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fef48ae8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.99, patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f90e1273",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scheduler as _scheduler\n",
    "reload(_scheduler)\n",
    "\n",
    "scheduler = _scheduler.ReduceLROnPlateau(optimizer, factor=0.99, patience=10, warmup=[[0.0003, 0.0007], [0.80, 0.80], 1000])\n",
    "# lrs = []\n",
    "# ms = []\n",
    "# for i in range(5000):\n",
    "#     scheduler.last_epoch = i\n",
    "#     lr, m = scheduler.step(0)\n",
    "#     ms.append(m)\n",
    "#     lrs.append(lr)\n",
    "    \n",
    "# plt.plot(lrs)\n",
    "# plt.show()\n",
    "# plt.plot(ms)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9075a6d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import scheduler as _scheduler\n",
    "# reload(_scheduler)\n",
    "\n",
    "# effectiveTotIters = totIters//accumulate\n",
    "# setSizeUp = effectiveTotIters//4\n",
    "\n",
    "# def scaleFn(cyclie):\n",
    "#     return 0.8**(cyclie-1)\n",
    "\n",
    "# points = [\n",
    "#     [0.001, 0.85, 0],\n",
    "#     [0.01,  0.95, 1*setSizeUp//2],\n",
    "#     [0.01,  0.85, 1],\n",
    "#     [0.01,  0.95, setSizeUp//2],\n",
    "#     [0.01,  0.85, 1],\n",
    "#     [0.01,  0.95, setSizeUp//2],\n",
    "#     [0.01,  0.85, 1],\n",
    "#     [0.01,  0.95, setSizeUp//2],\n",
    "#     [0.01,  0.85, 1],\n",
    "#     [0.001,  0.95, effectiveTotIters - 4*(setSizeUp//2) - 4],\n",
    "# ]\n",
    "\n",
    "# print()\n",
    "\n",
    "# scheduler = _scheduler.StepLR(optimizer, *list(zip(*points)))\n",
    "# print('scheduler.last_epoch', scheduler.last_epoch)\n",
    "\n",
    "# lrs = []\n",
    "# ms = []\n",
    "# for i in range(4*setSizeUp):\n",
    "#     scheduler.last_epoch = i\n",
    "#     lr, m = scheduler.get_lr_momentum()\n",
    "\n",
    "#     lrs.append(lr[0])\n",
    "#     ms.append(m[0])\n",
    "# plt.plot(lrs)\n",
    "# plt.show()\n",
    "# plt.plot(ms)\n",
    "# plt.show()\n",
    "\n",
    "# scheduler.last_epoch = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2262ef6e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# effectiveTotIters = totIters//accumulate\n",
    "# setSizeUp = effectiveTotIters\n",
    "\n",
    "# def scaleFn(cyclie):\n",
    "#     return 0.8**(cyclie-1)\n",
    "\n",
    "# scheduler = lr_scheduler.CyclicLR(\n",
    "#     optimizer,\n",
    "#     max_lr=0.01,\n",
    "#     base_lr=0.001,\n",
    "#     base_momentum = 0.8,\n",
    "#     max_momentum = 0.93,\n",
    "#     step_size_up = setSizeUp,\n",
    "#     step_size_down = effectiveTotIters - setSizeUp + 1000*effectiveTotIters,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "061fccaa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# effectiveTotIters = totIters//accumulate\n",
    "# setSizeUp = effectiveTotIters//4\n",
    "\n",
    "# def scaleFn(cyclie):\n",
    "#     return 0.8**(cyclie-1)\n",
    "\n",
    "# scheduler = lr_scheduler.CyclicLR(\n",
    "#     optimizer,\n",
    "#     max_lr=0.005,\n",
    "#     base_lr=0.0001,\n",
    "#     step_size_up = setSizeUp,\n",
    "#     step_size_down = effectiveTotIters - setSizeUp + effectiveTotIters,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7bc5237f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# effectiveTotIters = totIters//accumulate\n",
    "# setSizeUp = effectiveTotIters\n",
    "\n",
    "# def scaleFn(cyclie):\n",
    "#     return 0.8**(cyclie-1)\n",
    "\n",
    "# scheduler = lr_scheduler.CyclicLR(\n",
    "#     optimizer,\n",
    "#     max_lr=0.00025,\n",
    "#     base_lr=1e-6/2,\n",
    "#     base_momentum = 0.9,\n",
    "#     step_size_up = setSizeUp,\n",
    "#     step_size_down = effectiveTotIters - setSizeUp + 1000*effectiveTotIters,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4baa2bdb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python310\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "-------------------------------\n",
      "batchSize: 64\n",
      "TRAINING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 1835/1835 [1:11:32<00:00,  2.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timeTraining => 4292.369779586792\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "batchSize: 64\n",
      "TRAINING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 1835/1835 [1:08:00<00:00,  2.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timeTraining => 4081.035787343979\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import losses as _losses\n",
    "reload(_losses)\n",
    "\n",
    "def saveState(epoch, model, optimizer, iteration, lr, path):\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'iter': iteration,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_dict': optimizer.state_dict(),\n",
    "        'lr': lr\n",
    "    }, path)\n",
    "\n",
    "calcLoss = _losses.Detection(batchSize)\n",
    "\n",
    "def train(dataloader, epoch):\n",
    "    print('batchSize:', batchSize)\n",
    "    maxIters = dataloader.dataset.__len__() / batchSize\n",
    "    \n",
    "    print('TRAINING')\n",
    "    startTraining = time.time()\n",
    "    model.train()\n",
    "    letLastIterEndTime = None\n",
    "\n",
    "    after_lr = 0\n",
    "    lossCeValue = 0\n",
    "    lossDiValue = 0\n",
    "    lossBboxL1Value = 0\n",
    "    lossDIOUValue = 0\n",
    "    for i, (x, y, b, c) in enumerate(tqdm(dataloader)):\n",
    "      iter = i\n",
    "      x = x.to(device, dtype=torch.float)\n",
    "      _, dx, dy = y.shape\n",
    "      y = y.to(device)\n",
    "      c = c.to(device)\n",
    "      b = b.to(device, dtype=torch.float)\n",
    "\n",
    "      # bd8 = downScaler8c1(c)\n",
    "      # bd16 = downScaler8c1(c)\n",
    "      # bd32 = downScaler8c1(c)\n",
    "\n",
    "      c = c.unsqueeze(1).unsqueeze(1)\n",
    "      c = c.expand(-1, dx, dy, -1)\n",
    "      x = torch.abs(x - c)*(1/255)\n",
    "      inputTensor = x.permute(0, 3, 1, 2)\n",
    "\n",
    "      # Compute prediction error\n",
    "      with torch.autocast(device, enabled=amp):\n",
    "        predClass, predXY, predWH, predWhOriginal = model(inputTensor)\n",
    "        predClass = torch.squeeze(predClass, dim=1)\n",
    "        predXY = predXY.permute(0, 2, 3, 1)\n",
    "        predWH = predWH.permute(0, 2, 3, 1)\n",
    "        predWhOriginal = predWhOriginal.permute(0, 2, 3, 1)\n",
    "        \n",
    "        lossBBox, lossCe, lossDice, lossDIOU = calcLoss.loss(predClass, predXY, predWH, predWhOriginal, y, b)\n",
    "\n",
    "        lossBBox = lossBBox\n",
    "        lossDIOU = lossDIOU\n",
    "        lossCe = lossCe * 100\n",
    "        lossIter = lossBBox + lossCe + lossDice + lossDIOU\n",
    "#         \n",
    "        effectiveBatchSize = accumulate\n",
    "        loss = (lossIter)/effectiveBatchSize\n",
    "        \n",
    "        lossCeValue += lossCe.item()/effectiveBatchSize\n",
    "        lossDiValue += lossDice.item()/effectiveBatchSize\n",
    "        lossBboxL1Value += lossBBox.item()/effectiveBatchSize\n",
    "        lossDIOUValue += lossDIOU.item()/effectiveBatchSize\n",
    "\n",
    "      grad_scaler.scale(loss).backward()\n",
    "      torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)\n",
    "\n",
    "      with torch.no_grad():        \n",
    "        # maxTarget = torch.max(torch.flatten(y, 1), 1).values\n",
    "        # print('maxVals:', maxTarget)\n",
    "        flattentedPred = torch.flatten(predClass.detach(), 1 )\n",
    "        maxVal = (torch.sum(torch.isnan(predClass)) + torch.sum(torch.isnan(predXY))).item() + torch.sum(torch.isnan(predWH)).item()\n",
    "        if maxVal > 0:\n",
    "            print('OVERFLOW')\n",
    "            asd\n",
    "\n",
    "      before_lr = optimizer.param_groups[0][\"lr\"]\n",
    "\n",
    "      if (i%accumulate == 0 and i != 0):\n",
    "            \n",
    "        dataSaver.add({\n",
    "            'iter': i + maxIters*epoch,\n",
    "            'di': lossDiValue,\n",
    "            'ce': lossCeValue,\n",
    "            'bboxL1': lossBboxL1Value,\n",
    "            'lossDIOU': lossDIOUValue,\n",
    "            'lr': optimizer.param_groups[0][\"lr\"],\n",
    "            'momentum': optimizer.param_groups[0][\"momentum\"],\n",
    "            'maxVals': maxVal,\n",
    "        })\n",
    "\n",
    "    \n",
    "        grad_scaler.step(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 10)\n",
    "        grad_scaler.update()\n",
    "        \n",
    "        \n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        lr, m = scheduler.step(lossDIOU + lossDice)\n",
    "    \n",
    "        \n",
    "        lossDiValue = 0\n",
    "        lossCeValue = 0\n",
    "        lossBboxL1Value = 0\n",
    "        lossDIOUValue = 0\n",
    "        \n",
    "        del loss\n",
    "\n",
    "    saveState(epoch, model, optimizer, len(dataloader), before_lr, getCheckpointName(modelName, epoch))\n",
    "    print('timeTraining =>', time.time() - startTraining)\n",
    "\n",
    "\n",
    "    \n",
    "dataSaver = DataSaver(os.path.join(modelName, 'output')) \n",
    "\n",
    "\n",
    "model.train()\n",
    "for epoch in range(currentEpoch, currentEpoch + epochs):\n",
    "    print(f\"Epoch {epoch}\\n-------------------------------\")\n",
    "    trainDataloader = dataProvider.getDataLoader(batchSize)\n",
    "    train(trainDataloader, epoch)\n",
    "    # test(test_dataloader, model, loss_fn)\n",
    "\n",
    "currentEpoch = currentEpoch + epochs\n",
    "print(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
